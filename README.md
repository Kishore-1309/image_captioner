# Image Captioning Project ğŸ–¼ï¸âœï¸

Generate captions for images using a bridge between CLIP + GPT-2

## ğŸš€ Installation

```bash
# Clone repository
git clone https://github.com/Kishore-1309/image_captioner.git
cd image_captioner

# Install dependencies
pip install -r requirements.txt
```

## ğŸ› ï¸ Usage

### Extract Features
```bash
python -m image_captioner.feature_extractor \
  --image_dir ./data/images \
  --save_dir ./features
```

### Train Model
```bash
python -m image_captioner.train \
  --features ./features \
  --captions ./data/captions.csv
```

### Generate Captions
```bash
python -m image_captioner.generate \
  --image test.jpg \
  --model checkpoints/best_model.pt
```

## ğŸ“‚ Project Structure
```
project/
â”œâ”€â”€ image_captioner/      # Core package
â”œâ”€â”€ data/                 # Put your images here
â”œâ”€â”€ features/             # CLIP features storage
â””â”€â”€ checkpoints/          # Saved models
```

## ğŸ¤ Contributing
1. Fork the project
2. Create your branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“œ License
MIT